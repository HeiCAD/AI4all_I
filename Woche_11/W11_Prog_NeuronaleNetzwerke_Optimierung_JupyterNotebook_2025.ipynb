{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimierung von neuronalen Netzwerken\n",
   "id": "5b1c7b363339ed67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Erstellen und Konfigurieren eines neuronalen Netzes\n",
    "\n",
    "Wie gewohnt importieren wir zuerst alle Module, die wir in unserem Programm brauchen werden."
   ],
   "id": "2abd5da51ecd57c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importiere die nötigen Module\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import keras"
   ],
   "id": "9a2ac4c44df00318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In diesem Jupyter Notebook arbeiten wir wieder mit dem <a href=\"https://www.kaggle.com/competitions/titanic/overview\">_Titanic_</a> Datensatz, mit dessen Hilfe wir ein künstliches neuronales Netz zur Vorhersage der Überlebenschance der Passagiere erstellen und trainieren wollen.\n",
    "\n",
    "Nachdem wir den Datensatz geladen haben, nehmen wir einige Vorverarbeitungsschritte an diesem vor und teilen ihn in die Training- und die Testmenge auf.\n"
   ],
   "id": "c2e11fc07a5855b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lade den Titanic-Datensatz\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Speichere das zweite (Passagierklasse), das dritte (Geschlecht),\n",
    "# das fünfte (Anzahl der Geschwister/Ehepartner an Bord), das sechste (Anzahl der Eltern/Kinder an Bord),\n",
    "# das siebte (Passagiertarif (Britisches Pfund)),\n",
    "# und das zehnte (Mann/Frau/Kind) Merkmal in die Datenmatrix\n",
    "X = titanic[['pclass', 'sex', 'sibsp', 'parch', 'fare', 'who']]\n",
    "\n",
    "# Speichere das erste Merkmal (überlebt) als Zielmerkmal\n",
    "y = titanic[['survived']]\n",
    "\n",
    "# Damit unterdrücken wir einige Warnungen\n",
    "# (Empfehlung von Pandas)\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# Speichere im Merkmal 'who' nur die Information darüber, ob ein Passagier\n",
    "# ein Kind war, und benenne es in 'is_child' um\n",
    "X['who'] = np.where(X['who'] == 'child', 1, 0)\n",
    "X = X.rename(columns={'who': 'is_child'})\n",
    "\n",
    "# Umwandlung kategorischer Merkmale nach der One-Hot-Methode\n",
    "X = pd.get_dummies(X, columns=['sex'], dtype=float)\n",
    "y = pd.get_dummies(y, columns=['survived'], dtype=float)\n",
    "\n",
    "# Normalisiere die Merkmalswerte auf den Bereich [0,1]\n",
    "X_normalized = preprocessing.minmax_scale(X)\n",
    "\n",
    "# Teile den Datensatz in die Trainings- und die Testmenge auf\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, test_size=0.15, random_state=61)"
   ],
   "id": "56f0bcc770832ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wir implementieren zuerst ein sequenzielles Feed-Forward-Netz mit sieben Neuronen in der Eingabeschicht, die den sieben Merkmalen unseres Datensatzes entsprechen. Diese verknüpfen wir mit einer vollständig verbundenen Schicht mit fünf Neuronen und der _ReLU_-Aktivierungsfunktion. Die Ausgabeschicht unseres Netzes enthält zwei Neuronen, die den beiden Klassen _überlebt_ und _nicht überlebt_ entsprechen. Als Aktivierungsfunktion fungiert die _softmax_-Funktion. In der Zusammenfassung sehen wir, dass unser Modell nur 52 Optimierungsparameter hat.",
   "id": "df4377b1f5326087"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Aufbau eines neuronalen Netzes\n",
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(X_normalized.shape[1],)))\n",
    "model.add(keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(2, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ],
   "id": "9a00e3db2cb4dc12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bevor wir unser Modell trainieren können, müssen wir es noch konfigurieren. Die Methode dafür heißt zwar _compile()_, führt aber eher eine Konfiguration des Modells durch, bei der drei wichtige Einstellungen gesetzt werden. Als muss die Verlustfunktion angeben werden, die den Fehler zwischen den berechneten und den tatsächlichen Werten für das Zielmerkmal auf den Trainingsdaten berechnet. Für die Klassifikationsaufgaben mit zwei Klassen entscheidet man sich meistens für die _binary_crossentropy_. Als zweites müssen wir uns auf einen Optimierer festlegen. Dieser bestimmt, wie jeder Optimierungsparameter, also jedes Gewicht und jeder Bias-Term im Rahmen der Backpropagation angepasst wird, um den Fehler zu reduzieren. Wir entscheiden uns für den prominenten _Adamax_-Optimierer. Optional kann man noch eine Liste der Metriken angeben, die während des Trainings neben den Werten für die Verlustfunktion zusätzlich berechnet werden sollen. Wir sind an der Überwachung der _Accuracy_, also der Korrektklassifikationsrate, interessiert.",
   "id": "77938ae576870edd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Konfiguriere das Modell\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adamax(learning_rate= 0.001),\n",
    "              metrics=['accuracy'])"
   ],
   "id": "84ea982ee84a2d27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bei der Konfiguration des Modells werden die Optimierungsparameter mit zufälligen Werten initialisiert. Also eigentlich können mit dem Modell schon Vorhersagen erstellt werden, die natürlich nicht sinnvoll sein müssen, weil das Modell noch nicht trainiert wurde. Die Vorhersagen werden mit der Methode _predict()_ erstellt. Dieser übergeben wir die ersten fünf Beobachtungen des Testdatensatzes und vergleichen die berechneten Ergebnisse mit den tatsächlichen Werten des Zielmerkmals Überlebensstatus.",
   "id": "80d487ca1b59d23e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Erstelle eine Vorhersage für die ersten fünf Beobachtungen des Trainingdatensatzes\n",
    "# mit untrainiertem Modell\n",
    "print(model.predict(X_test[:5]))\n",
    "print(y_test[:5])"
   ],
   "id": "e380e53fd2c788e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training und Evaluation eines neuronalen Netzes\n",
    "\n",
    "Nachdem uns die Vorhersagen des untrainierten Modells nicht ganz überzeugt haben, trainieren wir unser neuronales Netzwerk anhand der Trainingsdaten. Dafür müssen wir einfach die Methode _fit()_ aufrufen. Dieser übergeben wir die Eingabemerkmale _X_train_ und die Zielmerkmale _y_train_. Außerdem müssen wir die Anzahl der _Epochen_ und die _batch_size_ angeben. Bei der _batch size_ handelt es sich um die Anzahl der Beobachtungen des Trainingssatzes, nach deren Verarbeitung die Optimierungsparameter des Modells neuberechnet werden. Ein guter Wert hierfür ist 32. Mit der Anzahl der Epochen legen wir fest, wie viel Mal unser neuronales Netz den gesamten Trainingsdatensatz bei dem Trainingsvorgang verarbeiten soll. Wir wollen nicht lange warten, deswegen setzen wir die Anzahl der Epochen auf fünf. Alle weiteren Parameter sind optional. Mit _validation_data_ können die Validierungsdaten übergeben werden, für die die Werte der Verlustfunktion und der Evaluationsmetriken beim Trainingsvorgang vom Modell mitberechnet werden, obwohl diese beim Training selbst nicht benutzt werden. Mit dem optionalen Parameter _verbose_ können wir festlegen, wie die Ausgabe des Trainingsvorgangs aussehen soll.\n",
    "\n",
    "Die Methode _fit()_ liefert eine _Historie_ zurück, die unter anderem die Werte der Verlustfunktion und der optionalen Bewertungsmetriken vom Ende jeder Epoche für den Trainings- und den Validierungsdatensatz enthält."
   ],
   "id": "1583a70893d11ca1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hier findet das eigentliche Training des Modells statt\n",
    "history = model.fit(x=X_train, y=y_train, epochs=5, batch_size=32,\n",
    "                    validation_data=(X_test, y_test), verbose=2)"
   ],
   "id": "a455ffe17ec973fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wenn du die Ergebnisse des Trainingsvorgangs lieber in einem Graphen betrachten möchtest, kannst du die Historie als ein DataFrame speichern und mithilfe des _plot()_-Befehls als Lernkurven anzeigen lassen.",
   "id": "ea411d407714977b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zeige die Ergebnisse für Loss und Accuracy pro Epoche in einem Graphen dar\n",
    "fig = pd.DataFrame(history.history).plot(figsize=(8, 5))"
   ],
   "id": "e65f85a8ebb9e88d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Unter- und Überanpassung\n",
    "\n",
    "Wenn die Accuracy-Werte sowohl für den Trainings- als auch den Validierungsdatensatz sehr niedrig sind, liegt es wahrscheinlich daran, dass das Modell zu einfach ist, um die in den Trainingsdaten enthaltenen Strukturen zu lernen. Wenn das auftritt, spricht man auch von _Underfitting_ bzw. _Unteranpassung_ des Modells. Um der Unteranpassung entgegen zu wirken, kann man zum Beispiel ein größeres Modell mit mehr Neuronen bzw. Optimierungsparametern verwenden und es länger trainieren."
   ],
   "id": "12c3d1de24803cff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Aufbau eines neuronalen Netzes\n",
    "model_2 = keras.Sequential()\n",
    "model_2.add(keras.Input(shape=(X_normalized.shape[1],)))\n",
    "model_2.add(keras.layers.Dense(units=15, activation=\"relu\"))\n",
    "model_2.add(keras.layers.Dense(2, activation = \"softmax\"))\n",
    "\n",
    "model_2.summary()"
   ],
   "id": "fc382e6347e95cce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Konfiguriere das Modell\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adamax(learning_rate= 0.001),\n",
    "              metrics=['accuracy'])"
   ],
   "id": "b87a2f1fc656f0f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hier findet das eigentliche Training des Modells statt\n",
    "history_2 = model_2.fit(x=X_train, y=y_train, epochs=100, batch_size=32,\n",
    "                    validation_data=(X_test, y_test), verbose=2)"
   ],
   "id": "dfc5ce6897d62de5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zeige die Ergebnisse für Loss und Accuracy pro Epoche in einem Graphen dar\n",
    "fig_2 = pd.DataFrame(history_2.history).plot(figsize=(8, 5))"
   ],
   "id": "cf3e906f925de338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unser Modell overfittet nicht, denn beim _Overfitting_ bzw. _Überanpassung_ würde die Accuracy für den Trainingsdatensatz mit der steigenden Epochenanzahl weiter ansteigen während sie für den Validierungsdatensatz entweder konstant bleiben oder sogar fallen würde. Dazu kommt es meistens, wenn das neuronale Netz zu mächtig ist bzw. zu viele Optimierungsparameter besitzt, die es dem Modell erlauben, sich an die zufälligen Schwankungen in den Trainingsdaten anzupassen, so dass das Modell nicht mehr generalisiert.",
   "id": "f14fd656a25e2e92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

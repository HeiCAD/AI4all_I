{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "94NZ-DC8ekBa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation und Import"
      ],
      "metadata": {
        "id": "94NZ-DC8ekBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmd6VMXreycF",
        "outputId": "2bd64297-c0d8-4324-89b8-f8970fcd001e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.9/401.9 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair) (0.8.10)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from flair) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.64.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from flair) (2.8.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.4.0)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair) (1.0.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from flair) (9.0.0)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 KB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from flair) (3.2.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair) (4.9.2)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from flair) (2022.6.2)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (2.2.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.8,>=1.5.0->flair) (4.4.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (6.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair) (0.2.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.12.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
            "Building wheels for collected packages: mpld3, sqlitedict, langdetect, pptree, overrides\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=8b3b5b1c42473cc70b9b3ee71dd135ed75ff989059a9510875f4c141bb3d9535\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/9f/9d/d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16869 sha256=be48733133be3b3e06ec168a8772a7aa4980d29400d94bebb8d740a9f8f60be9\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/c6/16/46e174009277f9bccdaa7215a243939d2f70180804b249bf3a\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=632e9cce96630d62f42c22031b1dc28ccb76877ad8e89a8a4430b3d2e5458533\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=8e6fa635c3dccd1a11e813c0103ed2e05fa83354a5720c4940723d990dd533c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/8b/30/5b20240d3d13a9dfafb6a6dd49d1b541c86d39812cb3690edf\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=2a9a93e96b70889c2e9193a718a3424b25ca3d9bac039c91b76a4fa14de8bc09\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
            "Successfully built mpld3 sqlitedict langdetect pptree overrides\n",
            "Installing collected packages: tokenizers, sqlitedict, sentencepiece, py4j, pptree, overrides, mpld3, janome, segtok, langdetect, importlib-metadata, ftfy, deprecated, conllu, wikipedia-api, konoha, hyperopt, huggingface-hub, transformers, bpemb, flair\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bpemb-0.3.4 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.12.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 tokenizers-0.13.2 transformers-4.26.1 wikipedia-api-0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "from flair.data import Sentence\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4owd0ZjOR7ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zusatzmaterial\n",
        "\n",
        "In diesem Abgschnitt gibt es einige Python-Methoden, die in der Haupterklärung nicht ausführlich behandelt wurden. Diese Methoden können jedoch nützlich sein, wenn Du tiefer in die Funktionsweise des Codes eintauchen möchtest."
      ],
      "metadata": {
        "id": "0A_DljvR8VyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_word_similarity(word_a, word_b, words, embeddings):\n",
        "    \"\"\"\n",
        "    Gibt die Kosinusähnlichkeit zwischen den Einbettungen zweier Wörter aus.\n",
        "    \n",
        "    Parameter:\n",
        "    wort_a (str): Das erste zu vergleichende Wort.\n",
        "    wort_b (str): Das zweite zu vergleichende Wort.\n",
        "    \n",
        "    Gibt zurück:\n",
        "    Keine\n",
        "    \n",
        "    Beispiel:\n",
        "    print_example_word_similarity(\"king\", \"man\")\n",
        "    # Ausgabe: Ähnlichkeit von König und Mann: 0.61\n",
        "    \"\"\"\n",
        "    embedding_a = [embeddings[words.index(word_a)]]\n",
        "    embedding_b = [embeddings[words.index(word_b)]]\n",
        "    print(\"Similarity of \", word_a, \" and \", word_b, \":\", *cosine_similarity(embedding_a, embedding_b).round(2))"
      ],
      "metadata": {
        "id": "brl9kLe_8bUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_suggestion(a, b, c, words, embeddings):\n",
        "    \"\"\"\n",
        "    Finde den Wortvorschlag für eine gegebene Gleichung \"a - b = x - c\".\n",
        "\n",
        "    Parameter:\n",
        "    a (str): Das erste Wort in der Gleichung.\n",
        "    b (str): Das zweite Wort in der Gleichung.\n",
        "    c (str): Das dritte Wort in der Gleichung.\n",
        "\n",
        "    words (list): Ein Liste mit Wörtern.\n",
        "    embeddings (list): Ein Liste mit Wort-Einbettungen.\n",
        "\n",
        "    Rückgabe:\n",
        "    str: Der Wortvorschlag für die Gleichung \"a - b = x - c\".\n",
        "    \"\"\"\n",
        "\n",
        "    lookup = dict(zip(words, embeddings))\n",
        "\n",
        "    # Ermittelt die Worteinbettungen für a, b, und c\n",
        "    vector_a = lookup.get(a)\n",
        "    vector_b = lookup.get(b)\n",
        "    vector_c = lookup.get(c)\n",
        "\n",
        "    # Erstellen eines Suchraums durch Entfernen von a, b und c aus dem Lookup-Dictionary\n",
        "    search_space = {\n",
        "        key: value for key, value in lookup.items()\n",
        "        if key not in [a, b, c]\n",
        "    }\n",
        "\n",
        "    # Berechnung der Einbettung für x durch Subtraktion von Vektor b von Vektor a und Addition von Vektor c\n",
        "    embedding_x = vector_a - vector_b + vector_c\n",
        "\n",
        "    # Erstellen Sie eine Liste aller Einbettungen im Suchraum.\n",
        "    embeddings = list(search_space.values())\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # Ermittlung der Kosinusähnlichkeit zwischen Einbettung_x und allen Einbettungen im Suchraum\n",
        "    similarities = cosine_similarity(embeddings, embedding_x.reshape(1, -1))\n",
        "\n",
        "    # Ermittelt den Index der ähnlichsten Einbettung\n",
        "    most_similar_index = np.argmax(similarities)\n",
        "\n",
        "    # Ermittelt das Wort, das der ähnlichsten Einbettung entspricht.\n",
        "    words = list(search_space.keys())\n",
        "    most_similar_word = words[most_similar_index]\n",
        "\n",
        "    # Ausgeben der Wortgleichung\n",
        "    print(a + \" - \" + b + \" = \" + most_similar_word + \" - \" + c)"
      ],
      "metadata": {
        "id": "V_8_EnJ39Ils"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_word_embeddings(words, embeddings):\n",
        "    \"\"\"\n",
        "    Nimmt eine Liste von Wörtern und ihre entsprechenden Einbettungen und visualisiert sie im 3D-Raum mit PCA.\n",
        "    Die Visualisierung erfolgt über ein Streudiagramm, in dem jedes Wort durch einen Punkt im 3D-Raum dargestellt wird, \n",
        "    und Wörter der gleichen Kategorie werden durch die gleiche Farbe dargestellt.\n",
        "\n",
        "    Parameter:\n",
        "    words (Liste): Eine Liste von Wörtern.\n",
        "    embeddings (Liste): Eine Liste der entsprechenden Worteinbettungen.\n",
        "\n",
        "    Rückgabe:\n",
        "    Keine\n",
        "    \"\"\"\n",
        "    word_embeddings = dict(zip(words, embeddings))\n",
        "\n",
        "    pca = PCA(n_components=3, random_state=12345)\n",
        "    pca_embeddings = pca.fit_transform(list(word_embeddings.values()))\n",
        "    \n",
        "    df_pca = pd.DataFrame(pca_embeddings)\n",
        "    df_pca['word'] = word_embeddings.keys()\n",
        "    df_pca = df_pca.rename(columns={0: 'x', 1: 'y', 2: 'z'})\n",
        "    \n",
        "    labels = list()\n",
        "    for word in [\"royal\", \"sex\", \"digits\", \"numbers\", \"car\", \"aircraft\", \"cities\", \"nations\"]:\n",
        "        for i in range(10):\n",
        "            labels.append(word)\n",
        "    df_pca['category'] = labels\n",
        "    \n",
        "    fig = px.scatter_3d(df_pca, x=\"x\", y=\"y\", z=\"z\", text=\"word\", color=\"category\", opacity=0.9, template=\"plotly_dark\")\n",
        "    fig.update_traces(marker_size = 5)\n",
        "    fig.update_scenes(xaxis_visible=False, yaxis_visible=False,zaxis_visible=False)\n",
        "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        fig.show(renderer='colab') \n",
        "    else:\n",
        "        fig.show(renderer='iframe') "
      ],
      "metadata": {
        "id": "VWVKmnaVBDfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text und Computer\n",
        "\n",
        "Wenn wir Texte verarbeiten, denken wir selten darüber nach, wie wir sie in einer für Computer verständlichen Form darstellen können. Text-Repräsentation und Wort-Einbettungen sind jedoch entscheidende Konzepte in der natürlichen Sprachverarbeitung, die uns dabei helfen, genau das zu tun!\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) ist ein fortschrittlicheres Modell zur Wort-Einbettung, das die Bedeutung von Wörtern anhand ihres Kontexts erfasst und sie in einem mehrdimensionalen Raum positioniert. Das mag sich zwar kompliziert anhören, aber es ist eigentlich ziemlich cool. Denn im Gegensatz zu One-Hot-Encodings kann BERT die Bedeutungen von Worten und deren Kontext besser darstellen. Das führt zu einer höheren Genauigkeit in der Sprachverarbeitung und hilft uns, noch besser zu verstehen, was in Texten wirklich vor sich geht.\n",
        "\n"
      ],
      "metadata": {
        "id": "LHv5TOefe8O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisiere BERT als Basis-Modell\n",
        "transformer = TransformerWordEmbeddings('bert-base-uncased')"
      ],
      "metadata": {
        "id": "kBBEbr-FRc1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aufbau von BERT\n",
        "\n",
        "Eine weitere faszinierende Eigenschaft von BERT ist, dass es bereits vortrainiert ist. Das bedeutet, dass es mit einer großen Menge an Texten trainiert wurde, um eine allgemeine Sprachkompetenz zu erwerben. Das ist vergleichbar mit dem Erlernen einer Sprache durch einen Menschen, der eine Vielzahl von Texten liest und somit ein besseres Verständnis für die Sprache entwickelt.\n",
        "\n",
        "Dabei erlernt BERT ein Vokabular aus etwa 30.000 Wörtern und Subworten. Das Modell verwendet das WordPiece-Vokabular, das bedeutet, dass lange Wörter in kleinere, häufig verwendete Teile zerlegt werden, die als Subwörter bezeichnet sind. Jedes Wort oder Subwort im Vokabular wird durch einen eindeutigen Vektor dargestellt, der im Modell verwendet wird, um die Bedeutung des Wortes zu erfassen.\n",
        "\n",
        "Ein Beispiel für ein solches Wort im WordPiece-Vokabular von BERT ist \"doghouse\". Da BERT mit einem unverarbeiteten Text trainiert wurde, wird das Wort in kleinere Subwörter aufgeteilt. Im Vokabular von BERT wird \"doghouse\" in zwei Subwörter unterteilt: \"dog\", und \"##house\" wobei das Symbol \"##\" anzeigt, dass das Subwort ein Teil eines längeren Wortes ist.\n",
        "\n",
        "Jedes dieser Subwörter hat eine eindeutige ID im Vokabular und wird durch einen entsprechenden Vektor repräsentiert. Wenn BERT mit einem Text arbeitet, der das Wort \"doghouse\" enthält, wird das Modell jedes Subwort erkennen und deren Vektoren kombinieren, um eine umfassende Repräsentation des Wortes zu erstellen.\n",
        "\n",
        "Zusätzlich erinnert die Architektur von BERT an den Aufbau von Neuronalen-Netzen. Es besteht aus vielen Schichten von sogenannten Encodern, die Informationen von vorherigen Schichten aufnehmen und verarbeiten, um kontextabhängige Repräsentationen von Wörtern zu erzeugen. Diese Architektur hat sich als sehr erfolgreich erwiesen und ist ein weiterer Beweis dafür, wie mächtig Neuronale-Netze in der heutigen Zeit sind."
      ],
      "metadata": {
        "id": "fXsC5TVyV1RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_vocabulary = transformer.tokenizer.vocab\n",
        "\n",
        "print(\"Größe Vokabular:\" , len(bert_vocabulary))\n",
        "print(\"Größe Einbettung:\", transformer.model.embeddings.word_embeddings)\n",
        "\n",
        "print(\"Id für das Wort 'dog':\", bert_vocabulary.get('dog'))\n",
        "print(\"Id für das Wort '##house':\", bert_vocabulary.get('##house'))\n",
        "print(\"Id für das Wort 'house':\", bert_vocabulary.get('house'))\n",
        "print(\"Id für das Wort 'doghouse':\", bert_vocabulary.get('doghouse'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtqqZ32YY2Lo",
        "outputId": "971e1f40-58d0-4bef-9d45-022b75597347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Größe Vokabular: 30522\n",
            "Größe Einbettung: Embedding(30522, 768, padding_idx=0)\n",
            "Id für das Wort 'dog': 3899\n",
            "Id für das Wort '##house': 4580\n",
            "Id für das Wort 'house': 2160\n",
            "Id für das Wort 'doghouse': None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Erhalte die Schichten des Transformers\n",
        "transformer_layers = transformer.model.encoder.layer\n",
        "print(\"Transformer Schichten:\", len(transformer_layers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBI_2nm8SDew",
        "outputId": "98798d8f-49af-4ca3-c3a4-62813a9cc05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Schichten: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# King - Man = X - Woman? Ein Beispiel für Textverarbeitung\n",
        "\n",
        "Ein klassisches Beispiel um das Konzept der Wort-Einbettungen und die Vorteile der BERT-Einbettungen zu demonstrieren ist das Rätsel: \"king - man = x - woman\". Dabei werden die Vektoren der Wörter \"king\", \"man\", \"woman\" und \"queen\" sinnbildlich in Bezug zueinander gestellt, um das Wort \"x\" zu erhalten. Die Intuition dahinter ist, dass die Differenz zwischen \"king\" und \"man\" die Eigenschaften repräsentiert, die einzigartig für das Wort \"king\" sind, und das Hinzufügen der Eigenschaften von \"woman\" zu dieser Differenz die Eigenschaften ergibt, die einzigartig für das Wort \"queen\" sind.\n",
        "\n",
        "Der Vorteil in der Verwendung von BERT-Einbettungen besteht darin, dass sie auf einem starken Sprachmodell basieren, und so in der Lage sind, anspruchsvollere und nuanciertere Beziehungen (Semantik) zwischen Wörtern erfassen."
      ],
      "metadata": {
        "id": "sP0AaA3IdNzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier siehst du einen Text aus jeweils 10 Worten für 8 Klassen, welche die 4 Kategorien Menschen, Zahlen, Fahrzeuge und Länder beschreiben.\n",
        "royal = \"royal monarchy crown queen king coronation throne palace majestic imperial\"\n",
        "sex = \"sex man woman diversity transgender masculinity femininity intersex LGBTQ gender\"\n",
        "\n",
        "digits = \"1 2 3 4 5 6 7 8 9 10\"\n",
        "numbers = \"one two three four five six seven eight nine ten\"\n",
        "\n",
        "car = \"car wheels engine speed road drive auto fuel vehicle emission\"\n",
        "aircraft = \"aircraft airbus boeing B747 A380 pilot cockpit wings takeoff landing\"\n",
        "\n",
        "cities = \"Beijing Tokyo London Paris Berlin Rome Madrid Warsaw Bangkok Bern\"\n",
        "nations = \"China Japan UK France Germany Italy Spain Poland Thailand Switzerland\"\n",
        "\n",
        "corpus = \" \".join([royal, sex, digits, numbers, car, aircraft, cities, nations])\n",
        "\n",
        "print(\"Text of all words:\", corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVp-5pZufZao",
        "outputId": "3a22fbfa-36d8-4ab4-ad12-17c72ea234b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text of all words: royal monarchy crown queen king coronation throne palace majestic imperial sex man woman diversity transgender masculinity femininity intersex LGBTQ gender 1 2 3 4 5 6 7 8 9 10 one two three four five six seven eight nine ten car wheels engine speed road drive auto fuel vehicle emission aircraft airbus boeing B747 A380 pilot cockpit wings takeoff landing Beijing Tokyo London Paris Berlin Rome Madrid Warsaw Bangkok Bern China Japan UK France Germany Italy Spain Poland Thailand Switzerland\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vom Wort zum Vektor\n",
        "\n",
        "Das Sentence-Objekt in Flair hilft dabei, Texte und Sätze als Vektoren darzustellen. Das ist hilfreich, um Texte zu analysieren und verschiedene Aufgaben in der Sprachverarbeitung zu lösen.\n",
        "\n",
        "Um Vektoren mit dem Sentence-Objekt zu erstellen, wird einfach die .embed() Methode aufgerufen. Dabei werden vorab trainierter BERT-Embedding-Vektoren verwendet, die dann für die jeweiligen Worte zusammengesetzt werden. Das ist ähnlich wie der Aufruf von fit_transform() in der sklearn Bibliothek, wo ein vorab trainiertes Modell die gegebenen Daten in eine andere Darstellung transformiert."
      ],
      "metadata": {
        "id": "J_vyR5KifmYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier wird aus unserem Korpus ein Satz aus einzelnen Worten gebildet.\n",
        "sentence = Sentence(corpus)\n",
        "\n",
        "# Hier wird für jedes einzelne Wort eine Einbettung erzeugen.\n",
        "transformer.embed(sentence)\n",
        "\n",
        "# Hier werden für die Wörter und Embeddings separate Listen erstellt.\n",
        "words = list()\n",
        "embeddings = list()\n",
        "\n",
        "for token in sentence:\n",
        "    word = token.text\n",
        "    embedding = token.embedding.numpy()\n",
        "    words.append(word)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "print(\"Beispiel Wort:\", words[0])\n",
        "print(\"Einbettung Wort:\", embeddings[0][:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNS6dKNvfecP",
        "outputId": "fa461371-aca9-4224-e6e2-48db51c0f06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiel Wort: royal\n",
            "Einbettung Wort: [ 0.8874518   0.17805542  0.58898306 -0.08429976]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantische Ähnlichkeit von Wörtern\n",
        "\n",
        "Semantische Ähnlichkeit ist eine Methode zur Messung der Ähnlichkeit zwischen Bedeutungen von Wörtern. Es hilft uns, in Anwendungen der Textverarbeitung besser zu machen, indem es uns ermöglicht, die Bedeutung von Wörtern zu verstehen und darzustellen.\n",
        "\n",
        "Daher kännen wir dieses Konzept verwenden, um die Gleichung \"king - man = c - woman\" mit BERT-Einbettungen zu lösen, indem die Ähnlichkeiten zwischen den Wörtern in der Gleichung verglichen werden.\n",
        "\n",
        "In diesem Gleichnis geht es darum, das Wort \"x\" zu finden, das dem Wort \"king\" am ähnlichsten ist, während es dem Wort \"man\" am unähnlichsten ist. In ähnlicher Weise sollte das Wort \"woman\" dem Wort \"man\" am ähnlichsten sein.\n",
        "\n",
        "Um das Wort \"x\" zu finden, können wir schlichtweg die Kosinus-Ähnlichkeit zwischen den BERT-Einbettungen der einzelnen Wortpaare in der Gleichung berechnen.\n",
        "Diese ist für eine solche Aufgabe gut geeignet, da sie den Kosinus des Winkels zwischen zwei Vektoren in einem hochdimensionalen Raum misst, wobei jeder Vektor die Einbettung eines Wortes darstellt.\n",
        "\n",
        "Die Kosinus-Ähnlichkeitsmetrik reicht von 0 bis 1, wobei 1 bedeutet, dass zwei Wörter genau dieselbe Bedeutung haben, und 0 bedeutet, dass die beiden Wörter völlig unterschiedliche Bedeutungen haben."
      ],
      "metadata": {
        "id": "7IEKEXjm6DX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier siehst Du, dass Worte ähnlich oder unähnlich sein können.\n",
        "print_word_similarity(\"king\", \"queen\", words, embeddings)\n",
        "print_word_similarity(\"man\", \"queen\", words, embeddings)\n",
        "print_word_similarity(\"woman\", \"queen\", words, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl0ayx247iO6",
        "outputId": "3b10bfad-bdf1-4250-9084-ad7669fd3bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity of  king  and  queen : [0.88]\n",
            "Similarity of  man  and  queen : [0.53]\n",
            "Similarity of  woman  and  king : [0.5]\n",
            "Similarity of  woman  and  queen : [0.54]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier siehst du, dass die Wort-Einbettungen einer gewissen Logik folgen.\n",
        "get_word_suggestion(\"king\", \"man\", \"woman\", words, embeddings)\n",
        "get_word_suggestion(\"3\", \"1\", \"2\", words, embeddings)\n",
        "get_word_suggestion(\"three\", \"one\", \"two\", words, embeddings)\n",
        "get_word_suggestion(\"aircraft\", \"wings\", \"wheels\", words, embeddings)\n",
        "get_word_suggestion(\"Germany\", \"Berlin\", \"Bangkok\", words, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-XwuY7C8_b9",
        "outputId": "7bef2903-4ff0-460f-b381-47c0beb4366d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "king - man = queen - woman\n",
            "3 - 1 = 4 - 2\n",
            "three - one = four - two\n",
            "aircraft - wings = vehicle - wheels\n",
            "Germany - Berlin = Thailand - Bangkok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier kannst du einen interaktiven Plot der Wöter und deren Bedeutung im 3-Dimensionalen betrachten.\n",
        "visualize_word_embeddings(words, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "WWDMLy0ZAHZQ",
        "outputId": "b278ca75-45bb-4d7d-8d51-7b7fcc1a920d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"466fae91-538d-468a-aa83-44b7b54898e8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"466fae91-538d-468a-aa83-44b7b54898e8\")) {                    Plotly.newPlot(                        \"466fae91-538d-468a-aa83-44b7b54898e8\",                        [{\"hovertemplate\":\"category=royal<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"royal\",\"marker\":{\"color\":\"#636efa\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"royal\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"royal\",\"monarchy\",\"crown\",\"queen\",\"king\",\"coronation\",\"throne\",\"palace\",\"majestic\",\"imperial\"],\"x\":[-2.899382558022423,-2.786459308734358,-1.713411002183949,-1.7042380313838275,-1.8513654448792178,-2.8536653401532783,-2.4817190355516767,-1.9748460181681389,-2.7570705551296,-1.9265506487332595],\"y\":[-4.679015873195694,-4.62609404521117,-3.95268016866431,-4.201435501923513,-4.150119953709769,-3.9672861581638386,-4.224131960623759,-2.9610425962163043,-3.936773177600205,-4.866544031967946],\"z\":[-4.843276172603376,-5.0347524151223855,-5.531513697781762,-4.745551104857437,-4.569102336095264,-4.641451010598889,-4.930538383489485,-3.2025509789998563,-2.89983828161283,-3.817922411099928],\"type\":\"scatter3d\"},{\"hovertemplate\":\"category=sex<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"sex\",\"marker\":{\"color\":\"#EF553B\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"sex\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"sex\",\"man\",\"woman\",\"diversity\",\"transgender\",\"masculinity\",\"femininity\",\"intersex\",\"LGBTQ\",\"gender\"],\"x\":[-0.9630938591670524,-1.094159061292358,-0.5528314000829249,-2.0019981227720836,-1.812896642566803,0.2259762748082816,0.4119033964697809,1.182002504597357,1.7843040954696723,2.390743490149358],\"y\":[-5.001355697692663,-4.492369874801876,-3.9532327247070564,-3.8827090652260736,-4.89188609958617,-2.922544208844597,-3.435895474736513,-2.9726367151739646,-3.8089879850139696,-3.4326888913567264],\"z\":[-3.7532970829793064,-2.2965867232679327,-2.479870825452782,-2.2484681385586565,-2.71301386394058,-3.4718907447902696,-2.748580752504724,-1.6566925144407016,-2.629794718480542,-2.5855745470988936],\"type\":\"scatter3d\"},{\"hovertemplate\":\"category=digits<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"digits\",\"marker\":{\"color\":\"#00cc96\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"digits\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],\"x\":[2.073497973901015,5.6565795189362476,6.696646005551363,8.345927300960659,9.610350609937008,9.420358916663982,10.223652451484877,9.704499641412383,8.559320038170956,5.56701639739556],\"y\":[-2.5606254000794224,0.14821008213763254,2.1330738380222205,2.4567549741291037,2.9056634769099725,2.9389777637765784,2.7316697156418996,3.1811466107555035,2.3701033512855836,1.2437129024929423],\"z\":[-2.9968841672392075,-3.9560077648184815,-3.5200849159974967,-2.9059979150547495,-2.885457062004135,-2.5061372309932444,-2.7409430731157856,-1.4508380544804949,-1.6571144940474334,-0.7899584617559178],\"type\":\"scatter3d\"},{\"hovertemplate\":\"category=numbers<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"numbers\",\"marker\":{\"color\":\"#ab63fa\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"numbers\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\"],\"x\":[6.5985646666540445,8.246785692485853,7.9068412672939825,8.63634123636685,9.388262276849343,8.79218998965135,9.000013750362816,7.531127119556648,5.815780956573926,1.692902841222355],\"y\":[0.698045412096721,1.8409750021150342,3.0792781062822425,2.713823194410533,2.887975218866082,3.3531236485610907,2.7668081555721424,2.631787015485374,1.3148499873928672,-2.0368269321156935],\"z\":[0.7360688404986706,0.895759015300523,1.0896801842947998,1.2702395923023562,1.6325318588722593,1.8926406600607764,1.9464777985769124,2.4635119729056965,1.8562405072725028,2.8016387039600446],\"type\":\"scatter3d\"},{\"hovertemplate\":\"category=car<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"car\",\"marker\":{\"color\":\"#FFA15A\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"car\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"car\",\"wheels\",\"engine\",\"speed\",\"road\",\"drive\",\"auto\",\"fuel\",\"vehicle\",\"emission\"],\"x\":[2.0990026057932436,-1.0782808161012896,-0.6352472032097081,-1.4838499211666993,-0.768762975593993,-0.6767132598595162,-0.7532650098848861,-0.6475010987400205,-1.4678638841194458,-2.698772086205689],\"y\":[-1.4482131182902465,-3.615000128271282,-4.204334742189259,-3.456162669548534,-2.9403978363512158,-3.1686154490584997,-3.8758947473050234,-3.890252486680045,-3.018380473349087,-4.1388914058344195],\"z\":[3.3554923716205844,2.863001594554079,4.566875461134126,2.66597373591386,3.3958698828583946,3.409659850600518,4.871858199059668,4.794375073014064,3.961884162537767,3.045806577229676],\"type\":\"scatter3d\"},{\"hovertemplate\":\"category=aircraft<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"aircraft\",\"marker\":{\"color\":\"#19d3f3\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"aircraft\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"aircraft\",\"airbus\",\"boeing\",\"B747\",\"A380\",\"pilot\",\"cockpit\",\"wings\",\"takeoff\",\"landing\"],\"x\":[-1.2074532474063278,-0.8437119154102838,-0.5987967915459154,-1.1371591673222583,0.22635314683934762,-1.040502955571276,-2.5299232809255536,-2.263831324638942,-0.7940141447820047,-2.156713204050367],\"y\":[-3.2137353077205617,-0.5049767418055413,-1.0964059773380914,-0.10271265464799549,-0.11271618360024017,-3.2927459279722147,-3.7945029927109006,-3.887841894087359,-2.4518487980391788,-2.372628938638153],\"z\":[5.725022569019364,6.167450974521684,6.713137360015701,5.118754240484499,5.905385207636187,6.075330001435647,3.7570004092892253,3.7276999088632796,6.05623879887918,4.266189864954632],\"type\":\"scatter3d\"},{\"hovertemplate\":\"category=cities<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cities\",\"marker\":{\"color\":\"#FF6692\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"cities\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"Beijing\",\"Tokyo\",\"London\",\"Paris\",\"Berlin\",\"Rome\",\"Madrid\",\"Warsaw\",\"Bangkok\",\"Bern\"],\"x\":[-4.945389648497109,-4.7225724537296285,-5.1326029817222745,-4.878658506469488,-5.523922313690304,-5.818789788202569,-4.86705264780216,-5.853312033449148,-5.863694206333269,-5.964693886070429],\"y\":[4.286177781430334,5.16273633349195,5.5122340762502,5.726494867764648,5.654486522199611,5.477825088879535,5.993708406075231,4.194261323365958,4.658927478466507,2.4665598694301645],\"z\":[0.7817072299433565,0.31928631994875406,-0.09704846909415377,0.04466149790691266,-0.034090588280613746,-0.34923632350320255,-0.34415018109899054,-0.2571810549867919,-0.803818735312174,-0.24606700744665744],\"type\":\"scatter3d\"},{\"hovertemplate\":\"category=nations<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"nations\",\"marker\":{\"color\":\"#B6E880\",\"opacity\":0.9,\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"nations\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"China\",\"Japan\",\"UK\",\"France\",\"Germany\",\"Italy\",\"Spain\",\"Poland\",\"Thailand\",\"Switzerland\"],\"x\":[-5.86702648560205,-5.000426858999783,-4.937207593766176,-4.804172891603819,-5.569810068499065,-5.123341703534968,-6.14259866705958,-4.846477101889321,-4.3900141821071,-5.379130831174904],\"y\":[4.854165238645304,5.398134595222985,4.267124902578946,5.7080889620622495,5.7560653256517345,5.316249552469694,5.569280299093396,5.6044970602215,3.527523448803206,3.012621422012387],\"z\":[-0.47575775957618377,-0.4095647592443602,-0.3453562851682629,0.15696519730758193,-0.4276874123629399,-0.6230418777547252,-1.3057236624255777,-1.634036398661083,-2.331129079966074,-2.4368361846089424],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"},\"visible\":false},\"yaxis\":{\"title\":{\"text\":\"y\"},\"visible\":false},\"zaxis\":{\"title\":{\"text\":\"z\"},\"visible\":false}},\"legend\":{\"title\":{\"text\":\"category\"},\"tracegroupgap\":0},\"margin\":{\"t\":0,\"l\":0,\"r\":0,\"b\":0}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('466fae91-538d-468a-aa83-44b7b54898e8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hausaufgaben\n",
        "\n",
        "Frage 1:\n",
        "Was ist ein Wort-Einbettungsmodell?\n",
        "\n",
        "    A) Eine Methode zur Messung der Ähnlichkeit zwischen Wörtern\n",
        "    B) Eine Methode zur Visualisierung der Verteilung von Wörtern in einem Vektorraum\n",
        "    C) Eine Methode zur Repräsentation der Bedeutung von Wörtern als Vektoren\n",
        "    D) Eine Methode zur Erstellung von Textkorpora aus Rohdaten\n",
        "    Antwort: C\n",
        "\n",
        "Frage 2:\n",
        "Welche Methode wird verwendet, um die Bedeutung von Wörtern in einem BERT-Modell zu lernen?\n",
        "\n",
        "    A) TF-IDF\n",
        "    B) Bag-of-Words-Modell\n",
        "    C) Word2Vec\n",
        "    D) Transformer-Netzwerke\n",
        "    Antwort: D\n",
        "\n",
        "Frage 3:\n",
        "Was ist der Unterschied zwischen BERT und Word2Vec?\n",
        "\n",
        "    A) BERT verwendet neuronale Netze, während Word2Vec ein statistisches Modell ist.\n",
        "    B) BERT kann bidirektionale Kontexte verwenden, während Word2Vec nur unidirektionale Kontexte verwenden kann.\n",
        "    C) BERT kann nur die Bedeutung von Wörtern, nicht aber deren Beziehungen, erfassen, während Word2Vec beides kann.\n",
        "    D) BERT verwendet lineare Algebra, während Word2Vec probabilistische Modelle verwendet.\n",
        "    Antwort: B"
      ],
      "metadata": {
        "id": "7RPrQkL-Dpr7"
      }
    }
  ]
}